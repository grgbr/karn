What it is all about
====================

This is mapred, a sample implementation of a map-reduce pattern for computing
occurrences of words in a given file.

Words are defined as a sequence of characters between 2 separators. These
separators are start and end of file, tabulations, spaces, punctuation, and end
of line.

Status
******

Proper unit testing should be implemented. Testing activity was reduced to the
minimum by comparing outputs generated by mapred and usual Linux tools.
No bugs found yet but I'm sure you will....

External API (scheduler) is too rigid. Flexible work units manipulation may
easily be implemented to support arbitrary mapping / reduce applicative logic.
A shared library might be of use.

Source needs cleanup. Complete Doxygen documentation.

Much room for improvements in terms of memory and algorithmic complexity.
Storage access performance was not really measured and usage of madvise (or
other VM / storage access system hints) should be investigated.
Analysis of realistic data sets should be carried out before any attempt to
optimize mapred.

No packaging... Anything you may think of.

Where to find
*************

Online at https://github.com/grgbr/mapred


Quick start guide
=================

Directory content
*****************

Doxyfile   - Doxygen source code documentation system setup file
LICENSE    - Licensing information appliable to all content
INSTALL    - Build instructions
Makefile   - Build rules
README     - This file
data/      - sample data sets
mapred.gdb - Some gdb macros to ease debugging
src/       - Directory where all mapred source files are located
test/      - Directory where unit test and validation scripts are located

How to run
**********

From top level directory, enter at command prompt:
$ mapred <my_dataset_file> <number of threads>

Mapred will output occurences of words found in input file passed as first
argument according to the following format:

	<word0>: <occurences of word0>
	<word1>: <occurences of word1>
	...
	<wordn>: <occurences of wordn>
	Total number of unique tokens: <total number of unique words>

Second argument specifies the number of threads to spawn in order to process the
whole input file.

Testing
*******

test/ directory includes the mapred.sh script which you may run to perform
functional validation of mapred. From the top-level directory, enter at command
prompt :
$ test/mapred.sh <my_dataset_file> <number of threads>

The script produces 2 files in the build/ directory named according to the
pattern below :
* <my_dataset_file_without_extension>_ref.txt (reference produced by standard
  Linux tools)
* <my_dataset_file_without_extension>_res.txt (result generated by mapred)

The script compares both files and displays any mismatches.


Design rationales
=================

Mapred was designed with the following kept in mind:
* must run on a standard x86/amd64 Linux box with reasonable up to date system ;
* with very limited software dependencies, i.e. libc6 only ;
* must be capable of handling input file larger than available physical memory ;
* best for average performance (against worst case) ;
* with maintainable source base.

Map and reduce
**************

In the following description, "mapping" operation depicts the initial work meant
to uniquely identify tokens found in an arbitrary sub-portion of the input file.
In the meantime, mapping phase also maintains a count of occurences for each
unique tokens crossed.
A reduce operation aggregates results produced by 2 mapping or 2 reduce
operations to generate a single result synthesizing unique tokens and count of
occurences of both related data subsets.

Typical mapred theory of operations:
* mmap entire file
* spawn threads according to what is given on command line
* give each thread a subset of the entire remap'ed file's address space to
  perform the initial mapping operation ;
* once its mapping phase is over, a thead is given results of initial mapping /
  subsequent reduce operations and performs reduction over 2 subset in a row
  untill no more subsets are available for processing ;
* the remaining reduced subset is the final result.

High level implementation description
*************************************

Mapred implements several kinds of software objects:
* token, token store and list,
* task, queue and operation,
* work and scheduler.

A token is a word (as described above) along with its count of occurences.
Tokens are ordered and indexed in a token store. During mapping phase, each
thread creates its own token store related to the file area it processes.
Token is built around a binary search tree to ease sorting and indexing.
At the end of the mapping phase, token store is flattened to produce a singly
linked list used as input by subsequent reduce operations.

Task is a simple wrapper around pthread. It uses queues to exchange operations
between themselves. Operation is the primary unit of data used to carry out
parallel processing (you might see an operation as some sort of asynchronous
mechanism).

Work and scheduler are the glue that tights together the above mentioned
objects. Work holds data and operation needed to process a given phase (map or
reduce). Basically, a work inherits from a task operation, allowing any task to
run the operation in a flexible and transparent manner.

The scheduler hands work units to tasks and monitors their completion in order
to enhance parallelization of processing. It may be seen as some kind of
applicative object implemeting arbritary logic to map work units over tasks.
Scheduler runs in main thread context and provides a primitive external
interface to main program.


Tradeoffs
*********

Mmap'ing the entire file is debatable since it is memory greedy and increases
pressure upon VM for very large files. However it is globally more efficient.
Main pros and cons :
* simplifies file access and code
* limit number of syscalls at the expense of additional page faults
* reduce number of memory copies.

Initial mapping phase is built around the libc6's binary search tree (tsearch
and co.) based upon a red-black tree implementation. This is a natural candidate
since tokens indexing and sorting are carried out in one single search / insert
operation.
Main pros and cons :
* simplifies code
* search and insert logarithmic complexity O(log n)
* lower memory space efficiency (tree nodes bookkeeping)
* not cache friendly (doesn't make use of cache locality)

Quicksort over an array based memory storage scheme would have been a suitable
alternative since it provides:
* much better memory space efficiency
* and much better cache awareness.

But at the cost of :
* complex memory heap management for the subsequent reduction phases since
  tokens are spread over multiple arrays
* additional code to index unique tokens
* worse algorithmic average complexity O(n log n)

Other alternatives were considered such as Radix tree but are rather complex to
implement in a few hours (without external library support).

Another key point relates to the way token strings are stored. Mapred keeps a
pointer to mmap'ed area (along with number of characters):
* less complex token allocation scheme
* no memory copies.
Another approach consist of storing the complete NULL ended token string in the
indexing data structure, mainly for better cache awareness.
In terms of space efficiency, both approaches heavily depends on average token
length. This needs careful analysis of data sets properties and is out of scope
within the given implementation time frame.

Regarding the reduction phase, mapred adopts a simpler singly linked list
scheme. Given that the mapping phase above outputs ordered unique token subsets,
the reduction operation is trivial. Mapred just needs to sort 2 already sorted
subsets and remove duplicate tokens.
A simple natural merge sort is implemented given that it provides:
* very simple code
* excellent performance over pre-sorted data
* in-place sorting (no memory overhead).


Basic performance evaluation
============================

To beging with, note that performance assessment has been carried out onto the
following setup:
* Linux 3.16 SMP x86_64 GNU/Linux
* mapred built with gcc-4.9 / libc6 2.19
* 8 Intel(R) Core(TM) i7-4770S CPU @ 3.10GHz
* with 8GB of memory

Note that data sets used for evaluation is to small to overflow available RAM.
This means results below should be taken with care since file's content is
already available in memory at the time tests are launched (I had no time to
setup a proper environment to completly trash memory each time a test is run).

2 scripts are provided in the test directory:
*  perf.sh monitors CPU counters using perf tool,
*  massif.sh profiles heap memory usage using valgrind.

Number of threads based analysis
********************************

Data set used was generated from a bunch open source projects by concatenating
all source files into one single file:
* 647407838 bytes long
* 1317538 unique tokens
* 72652612 words

Basic testing using perf shows the following key points:
* performances scale linearly with number of threads up to the number of
  available CPUs (8) then lightly decrease with additional threads,
* probably due to context-switching and task migration overhead since mapred was
  designed for very low contention between threads.

Here are a few meaningful numbers generated by the command below :
$ test/perf.sh /users/greg/build/dataset.txt <number_of_tasks> >/dev/null

# Task | # context |  # cpu     | # page  |    branch misses    | Time elapsed
       |  switches | migrations |  faults | (% of all branches) |  (seconds)
       |           |            |         |                     |
   1   |     329   |      13    |  28249  |       4.19%         | 37.124946522
   4   |     229   |      32    |  36304  |       4.10%         | 11.000484212
   8   |    2344   |      69    |  40846  |       4.39%         |  7.338153791
  16   |   74756   |   23911    |  46358  |       4.33%         |  9.657179567

Data set size and number of token based analysis
************************************************

Numbers below does not show immediate correlation between processing time and
number of tokens as we might expect. I suspect that the data set structure and
characteristics (such as unique tokens over total number of tokens ratio) deeply
influences processing performances.

Moreover, the smaller the file size, the higher the system / threading / binary
loading overhead.
Further investigation is needed with carefully crafted data sets to infer
meaningful conclusion.

Size (bytes) | # tokens | # unique | Time elapsed |  # tokens  | # unique tokens
             |          |  tokens  |   (seconds)  | per second |    per second
             |          |          |              |            |
  647407838  | 72652612 |  1317538 |  7.338153791 |  88224894  |     179546
   27982337  |  3456622 |   249026 |  0.361223202 |   9569214  |     689396
    1377129  |   269424 |    22735 |  0.033929137 |   7940786  |     670073
       5080  |      696 |      548 |  0.003026243 |    229988  |     181082

Memory consumption
******************

Valgrind massif tool allows to profile memory heap usage, giving us a good
indication as to memory requirements per processed tokens.
Test was performed upon the largest of data sets.


                                                                      Max heap
                                                                       (123MB)
Memory bytes
                                                                           |
                                                                           v
    MB
118.2^                                                                      :
     |                                                                  @@@#:
     |                                                             @@@@@@@@#::
     |                                                        :@:::@@@@@@@@#::
     |                                                    @@:@:@:: @@@@@@@@#::
     |                                                @::@@ :@:@:: @@@@@@@@#::
     |                                          @:::::@::@@ :@:@:: @@@@@@@@#::
     |                                       @@:@:: : @::@@ :@:@:: @@@@@@@@#::
     |                                    @@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
     |                                 :@@@@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
     |                           @@:@:::@ @@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
     |                        @@@@@:@:::@ @@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
     |                    @@::@@ @@:@:::@ @@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
     |                 @:@@ : @@ @@:@:::@ @@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
     |             :@::@:@@ : @@ @@:@:::@ @@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
     |         :@:::@: @:@@ : @@ @@:@:::@ @@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
     |       :::@: :@: @:@@ : @@ @@:@:::@ @@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
     |    ::::::@: :@: @:@@ : @@ @@:@:::@ @@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
     |  ::: ::::@: :@: @:@@ : @@ @@:@:::@ @@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
     | @::: ::::@: :@: @:@@ : @@ @@:@:::@ @@:@ :@:: : @::@@ :@:@:: @@@@@@@@#::
   0 +----------------------------------------------------------------------->Gi
     0                                                                   137.8
                         Number of processed instructions

Memory heap usage seems to grow linearly with number of unique tokens found in
the data sets. This is quite consistent with the way tokens are stored in
mapred (see design tradeoffs above).

As to stack usage, maximum consumption remains fairly limited to 4MB.

/usr/bin/time -v build/mapred /users/greg/build/dataset.txt 8 shows a rather
high RSS (i.e., complete shared and private physical memory usage) of 757216 KB.

Further investigation is needed to qualify memory consumption with respect to
data set structure and properties (mainly, token length, number of unique tokens
and overall number of tokens).

I/O operations
**************

As I was running out of time, I did not perform any testing against storage /
file I/Os operations. This is definitly something to measure in order to
evaluate mapred I/O behaviour. Perf would be a suitable tool for this (amongst
others).
